{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "\n",
    "**_Author: Jessica Cervi_**\n",
    "\n",
    "**Expected time = 2.5 hours**\n",
    "\n",
    "**Total points = 80 points**\n",
    "\n",
    "## Assignment Overview \n",
    "\n",
    "In this assignment, we will begin considering the second category of supervised learning problems from our machine learning lectures -- classification problems. Specifically, we will use `Scikit-Learn` to implement and evaluate classification models on the MNIST handwritten digit dataset. This is to serve as a reminder and expansion of some earlier work. Later, you will explore how to apply these machine learning ideas to graphs.\n",
    "\n",
    "This assignment is designed to build your familiarity and comfort coding in Python while also helping you review key topics from each module. As you progress through the assignment, answers will get increasingly complex. It is important that you adopt a data scientist's mindset when completing this assignment. **Remember to run your code from each cell before submitting your assignment.** Running your code beforehand will notify you of errors and give you a chance to fix your errors before submitting. You should view your Vocareum submission as if you are delivering a final project to your manager or client. \n",
    "\n",
    "***Vocareum Tips***\n",
    "- Do not add arguments or options to functions unless you are specifically asked to. This will cause an error in Vocareum.\n",
    "- Do not use a library unless you are expicitly asked to in the question. \n",
    "- You can download the Grading Report after submitting the assignment. This will include feedback and hints on incorrect questions. \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand the main concepts behind classification problems\n",
    "- Understand the difference between training and testing sets\n",
    "- Understand dummy variables and dummy classifiers\n",
    "- Measure the perormance of a classifier using the accuracy score or the confusion matrix\n",
    "- Use the implementation of Logistic Regression in `sklearn`\n",
    "- Use the decision tree classifier in `sklearn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "## Index: \n",
    "\n",
    "####  Machine Learning\n",
    "\n",
    "- [Question 1](#q01)\n",
    "- [Question 2](#q02)\n",
    "- [Question 3](#q03)\n",
    "- [Question 4](#q04)\n",
    "- [Question 5](#q05)\n",
    "- [Question 6](#q06)\n",
    "- [Question 7](#q07)\n",
    "- [Question 8](#q08)\n",
    "- [Question 9](#q09)\n",
    "- [Question 10](#q10)\n",
    "- [Question 11](#q11)\n",
    "- [Question 12](#q12)\n",
    "- [Question 13](#q13)\n",
    "- [Question 14](#q14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "In this assignment, we will work with different dataset to understand and consolidate some of main concept behind an important class of machine learning algorithms: classification problems.\n",
    "\n",
    "We will begin by splitting our data into a **training** and into a **testing** sets: the testing subset is for building your model. The testing subset is for using the model on unknown data to evaluate the performance of the model.\n",
    "Next, we learn how to use a dummy classifier. A **dummy classifier** is a type of classifier which does not generate any insight about the data and classifies the given data using only simple rules. The classifier’s behavior is completely independent of the training data as the trends in the training data are completely ignored and instead uses one of the strategies to predict the class label.\n",
    "It is used only as a simple baseline for the other classifiers i.e. any other classifier is expected to perform better on the given dataset. \n",
    "Next, we will learn how to measure the accuracy of a classiefier using two different metrics: the **accuracy score** and the **confusion matrix**.\n",
    "Finally, we will guide you through the usage of an important classifier implemented in the `sk-learn` library: Logistic Regression.\n",
    "\n",
    "\n",
    "As usual, we will begin by importing the librarries that we will be needing for this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Our standard data imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "#this is used to grade the assignment\n",
    "import numpy.testing as npt\n",
    "import pandas.testing as pdt\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In this assignment, we will work with a dataset from the library `sklearn`. The digit dataset dataset is made up of 1,797 8x8 images. Each image is an hand-written digit. In order to utilize an 8x8 figure like this, we’d have to first transform it into a feature vector with length 64.\n",
    "\n",
    "You can find more information about the dataset [here](https://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html) or by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "Input data shape: (1797, 64)\tTarget data shape: (1797,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.DESCR)\n",
    "\n",
    "# Extract data and targets as Numpy arrays\n",
    "X, y = digits.data, digits.target\n",
    "print('Input data shape: {}\\tTarget data shape: {}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "To get a better feel of what the input data is, let's extract a row of 64 numbers, reshape it into an $8\\times8$ array, and examine the resulting matrix by printing the numeric values & by plotting it as an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  2. 12.  9.  0.  0.  0.]\n",
      " [ 0.  0. 11. 15. 12.  5.  0.  0.]\n",
      " [ 0.  0. 15.  5.  0. 14.  0.  0.]\n",
      " [ 0.  2. 15.  1.  0.  9.  7.  0.]\n",
      " [ 0.  4. 10.  0.  0.  7.  8.  0.]\n",
      " [ 0.  0. 12.  0.  0.  8. 10.  0.]\n",
      " [ 0.  2. 15.  5. 10. 16.  1.  0.]\n",
      " [ 0.  0.  5. 14. 12.  4.  0.  0.]]\n",
      "y_130 = 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABE5JREFUeJzt3aGLHnQcx/Hf7USOgaggyECWxl24tLhg1CAYDILhwjBoMAkD2Qzbn2BTgzIYWAxiUrAIojJYPUFkxTBYsDhEEW6P/8AVYfvu7s3rFS88n+cJb35w5bu12WwW0HHmSX8B4NESNcSIGmJEDTGihpinHseHvnLmzeS/1Lf390b3Xvzs3tjW97/M/bbdt++MbZV99/DLreP+7qWGGFFDjKghRtQQI2qIETXEiBpiRA0xooYYUUOMqCFG1BAjaogRNcSIGmJEDTGihhhRQ4yoIUbUECNqiBE1xIgaYkQNMaKGGFFDzGM5u1P190f/jO5dPfft6N6Uw8uXxraev/nz2NZJ4aWGGFFDjKghRtQQI2qIETXEiBpiRA0xooYYUUOMqCFG1BAjaogRNcSIGmJEDTGihhhRQ4yoIUbUECNqiBE1xIgaYkQNMaKGGFFDjLM7/8Onu1+M7h1cvzK2NXme5tpvt8a2Pr55YWzrpPBSQ4yoIUbUECNqiBE1xIgaYkQNMaKGGFFDjKghRtQQI2qIETXEiBpiRA0xooYYUUOMqCFG1BAjaogRNcSIGmJEDTGihhhRQ4yoIUbUEHPqb2lt7+8Nrv04uLXWC9/cHds6Glta6/0f3hrb2rnx9NjWWmudv/HT6N5xvNQQI2qIETXEiBpiRA0xooYYUUOMqCFG1BAjaogRNcSIGmJEDTGihhhRQ4yoIUbUECNqiBE1xIgaYkQNMaKGGFFDjKghRtQQI2qIOfVndx7sPje29cadd8a21lrrpfuHo3tTdn6fO4Xz77MPx7ZOCi81xIgaYkQNMaKGGFFDjKghRtQQI2qIETXEiBpiRA0xooYYUUOMqCFG1BAjaogRNcSIGmJEDTGihhhRQ4yoIUbUECNqiBE1xIgaYk792Z2zX90e27rwwTNjW2utdW90bc7kKZyd8w/Gtk4KLzXEiBpiRA0xooYYUUOMqCFG1BAjaogRNcSIGmJEDTGihhhRQ4yoIUbUECNqiBE1xIgaYkQNMaKGGFFDjKghRtQQI2qIETXEiBpiTv3Zne39vbGtq+c+H9taa62Dy1fGtv56/c+xrbuXPhnbeu3iq2Nba611NLp2PC81xIgaYkQNMaKGGFFDjKghRtQQI2qIETXEiBpiRA0xooYYUUOMqCFG1BAjaogRNcSIGmJEDTGihhhRQ4yoIUbUECNqiBE1xIgaYkQNMaf+ltbR4a9jWwfX525brbXWtQ9vjW19/cfFsa2X33t3bOvs/dtjWyeFlxpiRA0xooYYUUOMqCFG1BAjaogRNcSIGmJEDTGihhhRQ4yoIUbUECNqiBE1xIgaYkQNMaKGGFFDjKghRtQQI2qIETXEiBpiRA0xW5vN5kl/B+AR8lJDjKghRtQQI2qIETXEiBpiRA0xooYYUUOMqCFG1BAjaogRNcSIGmJEDTGihhhRQ4yoIUbUECNqiBE1xIgaYkQNMf8B1alGkrGFyOQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 130\n",
    "im = X[k].reshape(8, 8)\n",
    "print(im)\n",
    "plt.imshow(im)\n",
    "plt.axis('off')\n",
    "print('y_{} = {}'.format(k, y[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Apparently, row 130 of the matrix `X` (remember, indexed from zero, this is the 131st row from the top), when reshaped, yields the image above. The corresponding entry of the target vector `y` is $0$ which means that this image is intended to represent the numeral $0$. Whether this is obvious depends on the handwriting of the original author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q01'></a>\n",
    "\n",
    "\n",
    "### Question 1:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Plot the image from row 100 of the matrix X. What numeral does this image represent?\n",
    "Assign your response as an integer to the identifier `ans_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an image of the digit 4.\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "\n",
    "k = 100\n",
    "y = digits.target\n",
    "### YOUR SOLUTION HERE\n",
    "ans_1 = y[k]\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "image_digit = X[k].reshape(8, 8)\n",
    "plt.imshow(image_digit)\n",
    "plt.show()\n",
    "print('This is an image of the digit {}.'.format(ans_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 01",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Target Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In classification problems, the labels (or targets) are *discrete* or *categorical* values (by contrast with regression problems). That being the case, we generally prefer the labeled data to be *balanced*; that is, we prefer having a uniform distribution of labels from which to build our models. For a binary classification problem (i.e., one with two classes), that would mean 50% of the data is from one class and 50% of the data from the other class. For a classification problem with $k$ classes, that would mean each class is represented in $(100 \\div k)$% of the data.\n",
    "\n",
    "Examining the target vector `y` for the MNIST digits data, it appears that each numeral from the sequence `0` through `9` occurs in a random sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3]\n"
     ]
    }
   ],
   "source": [
    "y = digits.target\n",
    "print(y[31:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    178\n",
       "1    182\n",
       "2    177\n",
       "3    183\n",
       "4    181\n",
       "5    182\n",
       "6    181\n",
       "7    179\n",
       "8    174\n",
       "9    180\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)\n",
    "y_ser = pd.Series(y)\n",
    "y_ser.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q02'></a>\n",
    "\n",
    "\n",
    "### Question 2:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your task here is to summarize how often each digit from `0` through `9` occurs in the vector `y`. The result be a Pandas Series with the integers `0` though `9` as the index (sorted in increasing order) and the corresponding counts as the data. Assign the result to the identifier `digit_counts`.\n",
    "\n",
    "(Hint: the Pandas Series method `value_counts` can do this easily, as can the Numpy function `unique`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    178\n",
      "1    182\n",
      "2    177\n",
      "3    183\n",
      "4    181\n",
      "5    182\n",
      "6    181\n",
      "7    179\n",
      "8    174\n",
      "9    180\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "y_ser = pd.Series(digits.target)\n",
    "digit_counts = y_ser.value_counts().sort_index()\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "print(digit_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 02",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Train/Test Split\n",
    "\n",
    "From question 2, it seems the MNIST digits data set is fairly balanced. Each of the 10 possible digits occurs roughly 180 times. As with regression problems, we want to divide the data into training and testing sets. The easiest way to do so is using the function `train_test_split` from the Scikit-Learn submodule `sklearn.model_selection` (you can consult the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to learn how to customize the behavior of this function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q03'></a>\n",
    "\n",
    "\n",
    "### Question 3:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your task here is to create a Pandas DataFrame with the digits from $0$ through $9$ in ascending order as the index and with two columns: `train_counts` and `test_counts`. The entries of each row, then, are the number of occurrences of that digit in the training target `y_train` and the testing target `y_test` respectively. Bind the DataFrame to the identifier `split_digit_counts`.\n",
    "\n",
    "That is, your final DataFrame should have these headings:\n",
    "               | train_counts | test_counts | \n",
    "       =======================================\n",
    "       |Digits |                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_counts</th>\n",
       "      <th>test_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mr. Hi</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Officer</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_counts  test_counts\n",
       "Mr. Hi             13            4\n",
       "Officer            12            5"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "train_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "test_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "df = pd.DataFrame({'train_counts':train_counts, 'test_counts':test_counts})\n",
    "split_digit_counts = df\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "split_digit_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 03",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Binary classification\n",
    "\n",
    "To begin, we will turn this into a binary classification problem.  We'll focus specifically on identifying all the occurrences of the digit `9`. For this simpler binary classification problem, we need to change every value in the vector `y` to `1` or `0` according to whether it is `9` or not. That is, replace every occurrence of `9` in the vector  `y` with the value `1` and replace all the other values with `0`. This process is called *binarization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n",
      "[0 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "qq = digits.target\n",
    "\n",
    "binary = np.where(qq == 9, 1,0)\n",
    "print(qq)\n",
    "print(binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q04'></a>\n",
    "\n",
    "\n",
    "### Question 4:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your task here is to create three new arrays: `y_bin`, `y_bin_train`, and `y_bin_test`. These will be binary vectors with `1`s replacing `9`s in `y`, `y_train`, and `y_test` respectively. All other entries will be replaced by `0`s.\n",
    "\n",
    "(Hint: The Numpy function `where` is very useful in this context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "y_bin = np.where(y == 9, 1, 0)\n",
    "y_bin_train = np.where(y_train == 9, 1, 0)\n",
    "y_bin_test = np.where(y_test == 9, 1, 0)\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 04",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### DummyClassifier\n",
    "\n",
    "To begin, we'll apply the built-in [`DummyClassifier` class from `sklearn.dummy`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) to set a baseline for performance of our future models.  This classifier does not actually use the feature matrix `X`. Classification decisions are made using the target vector `y` only. There are a few permissible strategies, but we'll start with the `'most_frequent'` strategy. That is, the `predict` method always returns the majority class. For our binary digit classification problem, this would be `0` (because the `1` classification is reserved for `9`s and most of the digits are not `9`s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Having applied the `fit` method to the training data, we can use the `predict` method to see how this estimator classifies the data. Unsurprisingly, it returns a vector of all `0`s (because that is the majority class for this data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_bin_pred = dummy.predict(X_test)\n",
    "print(y_bin_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We can compute the fraction of correct classifications using the method `score` with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of correct classifications is: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "score = dummy.score(X_test, y_bin_test)\n",
    "print('The fraction of correct classifications is: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Using `dummy.score` is equivalent to explicitly comparing the entries of `y_bin_pred` to `y_bin_test`, counting the number of correct classifications, and dividing by the number of classifications in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of correct classifications is: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "# This is the long way of computing the accuracy score\n",
    "correct_classifications = (y_bin_pred == y_bin_test)\n",
    "score = correct_classifications.sum() / len(correct_classifications)\n",
    "print('The fraction of correct classifications is: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For classification problems, a *confusion matrix* is a more detailed description of the accuracy of a classifier. It contains entries for the actual values as rows and predicted values as columns. This means we have:\n",
    "\n",
    "| $~$ | predicted 0 | predicted 1 |\n",
    "| ---- | ----------- | ---------- |\n",
    "| **actual 0** |  true negative | false positive |\n",
    "| **actual 1** |  false negative | true positive |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In *Scikit-Learn*, the `confusion_matrix` function takes as arguments the actual labels followed by the predicted labels (labeled in ascending order according to the class labels). From the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html):\n",
    "\n",
    "> `sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)`\n",
    ">\n",
    "> Compute a confusion matrix to evaluate the accuracy of a classification\n",
    ">\n",
    "> By definition a confusion matrix $C$ is such that $C_{i,j}$ is equal to the number of observations known to be in group $i$ but predicted to be in group $j$.\n",
    ">\n",
    "> Thus in a binary classification, the count of true negatives is $C_{0,0}$, false negatives is $C_{1,0}$, true positives is $C_{1,1}$, and false positives is $C_{0,1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q05'></a>\n",
    "\n",
    "\n",
    "### Question 5:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Generate the confusion matrix associated with the test data for this digits binary classification problem (i.e., computing which images correspond to the digit `9` and which do not). \n",
    "\n",
    "Use the training data (X_train, y_bin_train) to fit a `DummyClassifier` class instance to the training data as we did above. Then, construct a prediction from the test  input features X_test and, by comparing to the test labels y_bin_test, build the corresponding confusion matrix. Assign the resulting 2D Numpy array to the identifier `bin_confusion_mat`.\n",
    "\n",
    "You can do so explicitly or you can use the function `confusion_matrix` from `sklearn.metrics` according to your preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dum = DummyClassifier(strategy=\"most_frequent\")\n",
    "dum.fit(X_train, y_bin_train)\n",
    "pred = dum.predict(X_test)\n",
    "### YOUR SOLUTION HERE\n",
    "bin_confusion_mat = confusion_matrix(y_bin_test, pred)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 05",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[402,   0],\n",
       "       [ 48,   0]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(4,)\n",
      "402 0 48 0\n"
     ]
    }
   ],
   "source": [
    "mat = bin_confusion_mat\n",
    "print(mat.shape)\n",
    "rr = mat.reshape(4,)\n",
    "ww = mat.ravel()\n",
    "print(rr.shape)\n",
    "ww\n",
    "k,j,h,g = ww\n",
    "print(k,j,h,g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "The most basic way to assess our performance is to compare how many predictions we were right on out of the total number of observations.  We refer to this as **accuracy**. Using the diagram of our confusion matrix above, we have the formula\n",
    "\n",
    "$$\\text{accuracy} = \\frac{\\text{tp} + \\text{tn}}{\\text{tn} + \\text{tp} + \\text{fn} + \\text{fp}}$$\n",
    "\n",
    "where $\\text{tp}$ is the number of true positives, $\\text{tn}$ is the number of true negatives, $\\text{fp}$ is the number of false positives, and $\\text{fn}$ is the number of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q06'></a>\n",
    "\n",
    "\n",
    "### Question 6:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your task here is to compose a function `accuracy_score` that implements the preceding formula. The input to the function is a (previously computed) confusion matrix and the output is an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def accuracy_score(confusion_matrix):\n",
    "    '''\n",
    "    This function takes in a confusion matrix \n",
    "    (from a binary classification problem)\n",
    "    and returns the accuracy score.\n",
    "    '''\n",
    "    #Sklearn:\n",
    "    #In the binary case, we can extract true positives, etc as follows:\n",
    "    #tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "    tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "    \n",
    "    return (tp + tn)/(tn + tp + fn + fp)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 06",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We move on now to estimators for classification problems that actually use the input data (unlike the `DummyClassifier`).  To begin, let's examine the [`LogisticRegression` estimator](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression). In order to determine a logistic regression model, a nonlinear system of equations needs to be solved iteratively. Thus, when we instantiate the estimator, we can specify the solver and the maximum number of iterations. For instance:\n",
    "\n",
    "```python\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> clf = LogisticRegression(solver='newton-cg', max_iter=1000)\n",
    "```\n",
    "Don't worry about what these particular optional parameters mean. Although other choices exist, we'll use these for now.\n",
    "\n",
    "As with other *Scikit-Learn*'s estimator classes, the `.fit` and `.predict` methods are used to construct the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver = 'newton-cg', max_iter=1000)\n",
    "clf.fit(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred[335:350] # Some zeros, some ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For classifications, we can also access the *probabilities* of correctness. That is, for a given observation (i.e., row of `X`), there is not only the class prediction (i.e., corresponding row of `y`), but there are associated *probabilities of that observation belonging to each class*. These probabilites are accessible by the method `predict_proba`. For each observation, this returns a row vector of nonnegative values that sum to 1 where the entry in column $k$ is the probability of belonging to class $k$. Thus, for this binary classification problem, after fitting a classifier (e.g., `LogisticRegression`) to the training data, the method `predict_proba` returns an $n_{\\text{test}}\\times2$ matrix of probabilities (where $n_{\\text{test}}$ is the number of observations in the testing set) whose rows all sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99999959e-01, 4.11043147e-08],\n",
       "       [9.99999993e-01, 7.12386042e-09],\n",
       "       [2.96744038e-05, 9.99970326e-01],\n",
       "       [9.99360858e-01, 6.39141990e-04],\n",
       "       [1.00000000e+00, 2.37518820e-13],\n",
       "       [9.99999999e-01, 8.60033381e-10],\n",
       "       [1.00000000e+00, 8.11464179e-20],\n",
       "       [1.00000000e+00, 1.32125471e-18],\n",
       "       [1.00000000e+00, 1.04445521e-13],\n",
       "       [9.99833299e-01, 1.66700962e-04],\n",
       "       [6.20630112e-06, 9.99993794e-01],\n",
       "       [9.99999998e-01, 1.91024017e-09],\n",
       "       [8.90698395e-06, 9.99991093e-01],\n",
       "       [9.99999910e-01, 9.00991657e-08],\n",
       "       [1.00000000e+00, 7.59892337e-11]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 0: probability of class 0\n",
    "# Column 1: probability of class 1\n",
    "clf.predict_proba(X_test)[335:350]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Here we have constructed a function `prob_table` encapsulating the previous computations. This function accepts the test data `X_test`, and `y_test`, followed by a classifier (e.g., `DummyClassifier`, `LogisticRegression`, etc.) as arguments. The classifier should already have been fit to data (e.g., `classifier.fit(X_train, y_train)` should already have been invoked). The result returned should be a DataFrame:\n",
    "\n",
    "\n",
    "\n",
    "|  $~$ | prob_0 | prob_1 | predicted_value | actual_value |\n",
    "| ---- | ----------- | ----------- | ------------------ | ------------ |\n",
    "| **0** |  $p_0$ | ($1-{}$ $p_0$) | $y^{\\text{pred}}_0$ | $y^{\\text{test}}_{0}$ |\n",
    "| **1** |  $p_1$ | ($1-{}$ $p_1$) | $y^{\\text{pred}}_1$ | $y^{\\text{test}}_{1}$ |\n",
    "| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prob_table(X_test, y_test, classifier):\n",
    "    '''\n",
    "    This function takes in a test set X_test, y_test,\n",
    "    and a classifier (that has been fit to data)\n",
    "    It returns a DataFrame with columns below:\n",
    "    | probab_0 | probab_1 | predicted_value | actual_value |\n",
    "    ========================================================\n",
    "    '''\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    probabilities = classifier.predict_proba(X_test)\n",
    "    df = pd.DataFrame()\n",
    "    for k in range(probabilities.shape[1]):\n",
    "        df['probab_{}'.format(k)] = probabilities[:,k]\n",
    "    df['predicted_value'] = y_pred\n",
    "    df['actual_value'] = y_test\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Standardizing the data\n",
    "\n",
    "Prior to fitting a given estimator to data, we usually want to *standardize* the data. This is usually done by replacing features with their respective *z-scores*. That is, we translate and rescale the data so that the $k$th feature $x_k$ is replaced according to the substitution $x_k \\leftarrow (x_k - \\mu_k) / \\sigma_k$ where $\\mu_k$ is the (empirical) mean of the $k$th feature and $\\sigma_k$ is the (empirical) standard deviation of the $k$th feature. When working with testing and training data sets, the values of $\\mu_k$ and $\\sigma_k$ are determined using the training data and those same values are used to standardize the test data when validating the resulting estimator.\n",
    "\n",
    "All of the above can be achieved using the [`StandardScaler` class from `sklearn.preprocessing`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instances of this class can accept a matrix of observations and return the corresponding z-scores down each column (remember, the rows are observations and the columns are the features). The resulting `StandardScalar` object, once fit to the training data, can be used to transform the testing data as well.\n",
    "\n",
    "Here's an example of how to use the `StandardScaler` class to transform data:\n",
    "```python\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> data = np.array([[0, 0], [0, 1], [1, 1], [1, 1]]) # Create 4x2 array of data\n",
    ">>> scaler = StandardScaler() # Instantiate StandardScaler object\n",
    ">>> scaler.fit(data)          # Use columns from data to define transformation\n",
    ">>> print(scaler.mean_)       # means of 2 columns from data\n",
    "[0.5  0.75]\n",
    ">>> print(scaler.var_)        # variances of 2 columns from data\n",
    ">>> print(scaler.transform(data))  # Applying transformation to data\n",
    "[[-1.         -1.73205081]\n",
    " [-1.          0.57735027]\n",
    " [ 1.          0.57735027]\n",
    " [ 1.          0.57735027]]\n",
    ">>> print(scaler.transform([[2, 2]]))  # Applying transformation to new observation\n",
    "[[3.         2.88675135]]\n",
    "```\n",
    "As an alternative to applying the methods `fit` and then `transform`, there is a `fit_transform` method that combines the two into a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q07'></a>\n",
    "\n",
    "\n",
    "### Question 7:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Your task here is to standardize the data using the `StandardScalar` class as above. You'll define the transformation to the training feature matrix `X_train` with the `fit` method. You'll apply the resulting transformation to `X_train` and `X_test` to yield standardized data for both the training and testing sets. From there, you'll define a `LogisticRegression` classifier and fit that to the standardized training feature matrix and the binary labels `y_bin_train`. Set the arguments `max_iter` equal to 1000 and `solver` equal to `newton-cg` inside the `LogisticRegression` classifier.\n",
    "\n",
    "Finally, you'll use the function `prob_table` from above to create a DataFrame `prob_table_standardized` that shows the classifications using the standardized test data as compared to the actual classifications with their corresponding probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lr_scale = StandardScaler()\n",
    "std_train = lr_scale.fit_transform(X_train)\n",
    "std_test = lr_scale.transform(X_test)\n",
    "lr_clf = LogisticRegression(max_iter = 1000, solver = 'newton-cg')\n",
    "lr_clf.fit(std_train, y_bin_train)\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "prob_table_standardized = prob_table(std_test, y_bin_test, lr_clf)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 07",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probab_0</th>\n",
       "      <th>probab_1</th>\n",
       "      <th>predicted_value</th>\n",
       "      <th>actual_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011709</td>\n",
       "      <td>0.988291</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.996751</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.833471</td>\n",
       "      <td>0.166529</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   probab_0  probab_1  predicted_value  actual_value\n",
       "0  0.999113  0.000887                0             0\n",
       "1  0.011709  0.988291                1             1\n",
       "2  0.996751  0.003249                0             0\n",
       "3  0.833471  0.166529                0             0\n",
       "4  0.999996  0.000004                0             0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_table_standardized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Using Pipelines\n",
    "\n",
    "Many modeling tasks involve the combination of preprocessing steps that are fed into an estimator.  As such, *Scikit-Learn* comes with a handy `Pipeline` module that allows us to combine *transformers* (e.g., `StandardScaler`) and *estimators* (e.g., `LinearRegression`, `LogisticRegression`, etc.) in a single object.  The `Pipeline` expects a sequence of transformers -- objects that have methods `fit`, `transform`, & `fit_transform` -- and ends with an estimator -- objects that have methods `fit`, `predict`, & `fit_predict`.  For example, we could have a `Pipeline` that scales our data and subsequently fits a `LogisticRegression` model as follows:\n",
    "\n",
    "```python\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression(max_iter=1000, solver='newton-cg'))])\n",
    "```\n",
    "\n",
    "Notice that the `Pipeline` object is instantiated using a list of tuples. All the tuples consist of a string identifier followed by an instance of one of Scikit-Learn's transformer classes except the last tuple (which has an estimator class instance instead). The resulting `Pipeline` object behaves like an estimator (i.e., it has methods `fit`, `predict`, and `fit_predict`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q08'></a>\n",
    "\n",
    "\n",
    "### Question 8:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your task here is to construct a `Pipeline` object as described above for a binary classification problem. We'll load and prepare the data `X` and `y` for you (`y` is a binary vector distinguishing the digit `3` from all other digits).\n",
    "\n",
    "Construct a pipeline object called pipe combining a StandardScaler transformation with a LogisticRegression estimator. Fit the resulting pipelined estimator's to the training data `X_train` and `y_train` (provided) and construct a vector `y_pred` using the predict method of the Pipeline.\n",
    "\n",
    "Be sure your solution binds a suitable Pipeline object to the identifier `pipe` and a `Numpy` array to the identifier `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "digits = load_digits()\n",
    "X, y = digits.data, np.where(digits.target==3, 1, 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "### YOUR SOLUTION HERE\n",
    "pipe = Pipeline([('scaler',StandardScaler()),('model',LogisticRegression(max_iter = 1000, solver = 'newton-cg'))])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 08",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "As with linear regression, we can use *regularization* in conjunction with logistic regression. That is, we can modify the objective function being minimized to construct the estimator with a penalty term. From the [Scikit-Learn User guide](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression):\n",
    "\n",
    "> As an optimization problem, binary class $L2$ penalized logistic regression minimizes the following cost function:\n",
    ">    $$\\min_{w,c} \\frac{1}{2}w^Tw + C\\sum_{i=1}^{n}\\log\\left(\\exp\\left(-y_i\\left(X_{i}^{T}w+c\\right)\\right)+1\\right).$$\n",
    "> Similarly, $L1$ regularized logistic regression solves the following optimization problem\n",
    ">    $$\\min_{w,c} \\left\\|w\\right\\|_{1} + C\\sum_{i=1}^{n}\\log\\left(\\exp\\left(-y_i\\left(X_{i}^{T}w+c\\right)\\right)+1\\right).$$\n",
    "> Note that, in this notation, it's assumed that the observation $y_i$ takes values in the set $\\{-1,1\\}$ at trial.\n",
    "\n",
    "Ignoring the mathematical details of all the terms in these objective functions, the regularization parameter in the Scikit-Learn `LogisticRegression` estimator is labeled `C` consistent with the parameter $C$ in the equations above. Loosely speaking, $C$ controls the relative importance of the penalty term (the terms $w^Tw$ or $\\|w\\|_1$ in each of the objective functions) and the unpenalized objective (the expression preceded by $C$). So, when $C$ is small, the coefficients in $w$ that determine the logistic regressor are penalized more strongly forcing the penalty term to be smaller.\n",
    "\n",
    "In practice with *Scikit-Learn*, we instantiate a `LogisticRegression` instance using the keyword option `C`. For our purposes, this means (when we use the `max_iter` and `solver` parameters as before) the following code for instantiation:\n",
    "\n",
    "```python\n",
    ">>> clf = LogisticRegression(C=100, max_iter=1000, solver='newton-cg')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Below, we have defined a function ,  `log_reg_gridsearch`, that accepts as input arguments the input feature matrix `X`, the target vector `y`, and a list `C_vals` of positive scalar values (for the regularization parameter $C$ in regularized logistic regression).\n",
    "\n",
    "The input data is split into training & test sets with a fixed random parameter 42.\n",
    "\n",
    "This function loops over the values `c_val` within the list `C_vals`. For each value, it:\n",
    "  - instantiate a `Pipeline` object with a `StandardScaler` object followed by a `LogisticRegression` object\n",
    "  \n",
    "  - instantiate the `LogisticRegression` estimator with keyword arguments `solver='newton-cg'`, `max_iter=1000`, and `C=c_val`.\n",
    "  \n",
    "- fit the `Pipeline`  to the training data `X_train` & `y_train`\n",
    "\n",
    "- uses the `Pipeline` to predict target values from the test data `X_test`\n",
    "\n",
    "The result returned by the function is a DataFrame with two columns: `'C'` and `'accuracy'`. The column `'C'` contains the elements of the input list `C_vals`. The column `'accuracy'` contains the corresponding accuracy score as computed in the loop just described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def log_reg_gridsearch(X, y, C_vals):\n",
    "    '''\n",
    "    Input: predictor X values, target y values, and a list of\n",
    "    values for C in the LogisticRegression estimator.\n",
    "    \n",
    "    Output: DataFrame with accuracy scores & C values\n",
    "    '''\n",
    "    # DO NOT CHANGE THE LINE BELOW\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    results = []\n",
    "    for c_val in C_vals:\n",
    "        kwargs = {'solver':'newton-cg', 'max_iter':1000, 'C':c_val}\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression(**kwargs))])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        results.append(accuracy_score_T(confusion_matrix(y_test, y_pred)))\n",
    "    df = pd.DataFrame({'C':C_vals, 'accuracy': results})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "Now, we turn to a different classifier -- the `DecisionTreeClassifier`.  The aim is to compare its performance to that of our `LogisticRegression` classifier. As with `LogisticRegression` and other estimators, the specific performance and behavior of the `DecisionTreeClassifier` can be tuned by specifying certain *hyperparameters* upon instantiation. Some useful hyperparameters for the `DecisionTreeClassifier` are:\n",
    "\n",
    "* `criterion`: `'gini'` (default) for the Gini impurity or `'entropy'` for the information gain\n",
    "* `max_depth`: depth of tree (default `None`; expands until all nodes are pure)\n",
    "* `min_samples_split`: minimum number of samples required to split a node (default 2)\n",
    "\n",
    "We also would like to search over different hyperparameters relevant to this classifier as well.  In the problem below, you are asked to fit a `DecisionTreeClassifier` and search over the three hyperparameters listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Here, we build a function `decision_tree_gridsearch` similar to `log_reg_gridsearch`. The function `decision_tree_gridsearch` accepts as input arguments the input feature matrix `X`, the target vector `y`, a list `criteria` of strings (either `'gini'` or `'entropy'` or both), a list `depths` of positive integers, and a list `min_splits` of positive integers.\n",
    "\n",
    "The input data is split into training & test sets with a fixed random parameter 42.\n",
    "\n",
    "This function will loop over all the hyperparameter values within the three input lists. For each hyperparameter combination, it:\n",
    "   - Instantiate a `DecisionTreeClassifier` object\n",
    "   \n",
    "   - Instantiate the `DecisionTreeClassifier` estimator with hyperparameters `criterion`, `max_depth`, and`min_samples_split` determined from the input\n",
    "   \n",
    "   - Fit the `DecisionTreeClassifier` to the training data `X_train` & `y_train`\n",
    "   \n",
    "   - Uses the `DecisionTreeClassifier`  to predict target values from the test data `X_test`\n",
    "\n",
    "\n",
    "The result returned by the function is a DataFrame with four columns: `'criterion'`, `'max_depth'`, `'min_samples_split'`, and `'accuracy'` . The first three columns contain all combinations of the hyperparameter values from the input lists. The column `'accuracy'` contains the corresponding accuracy score as computed in the loop just described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def decision_tree_gridsearch(X, y, criteria, max_depths, min_splits):\n",
    "    '''\n",
    "    Input: predictor X values, target y values, and lists criteria,\n",
    "           max_depths, & min_splits of hyperparameter values for\n",
    "           the DecisionTreeClassifier.\n",
    "    \n",
    "    Output: DataFrame with accuracy scores & hyperparameter values;\n",
    "            column headings as follows:\n",
    "            | 'criterion' | 'max_depth' | 'min_samples_split' | 'accuracy' |\n",
    "            ----------------------------------------------------------------\n",
    "    '''\n",
    "    # DO NOT CHANGE NEXT LINE\n",
    "    X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, random_state=42)\n",
    "    results = []\n",
    "    crit = []\n",
    "    mdepth = []\n",
    "    minspl = []\n",
    "    for criterion in criteria:\n",
    "        for max_depth in max_depths:\n",
    "            for min_samples_split in min_splits:\n",
    "                kwargs = {'criterion':criterion,\n",
    "                          'max_depth':max_depth,\n",
    "                          'min_samples_split':min_samples_split}\n",
    "                clf_ = DecisionTreeClassifier(**kwargs)\n",
    "                clf_.fit(X_train_, y_train_)\n",
    "                results.append(clf_.score(X_test_, y_test_))\n",
    "                crit.append(criterion)\n",
    "                mdepth.append(max_depth)\n",
    "                minspl.append(min_samples_split)\n",
    "    df = pd.DataFrame({'criterion':crit, 'max_depth':mdepth,\n",
    "                       'min_samples_split':minspl, 'accuracy': results})\n",
    "    df = df.sort_values(by=['criterion', 'max_depth', 'min_samples_split']).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning on Graphs\n",
    "\n",
    "\n",
    "In this part of the assignment, we investigate how to apply some basic machine learning concepts to graph structures. This involves structuring data from a graph in familiar tabular format. We will begin by looking at a classic graph from Zachary's Karate Club. This is built in to `networkx` and has an attribute called `club` associated with each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#create the graph \n",
    "K = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Zachary's Karate Club\n",
      "Type: Graph\n",
      "Number of nodes: 34\n",
      "Number of edges: 78\n",
      "Average degree:   4.5882\n"
     ]
    }
   ],
   "source": [
    "#print info\n",
    "print(nx.info(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView((0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'club': 'Mr. Hi'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking node structure\n",
    "K.nodes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Hi'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node attributes\n",
    "K.nodes[5]['club']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Hi'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the value\n",
    "K.nodes[0]['club']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, we can construct a `DataFrame` object using the nodes as indices.  Then, we will map the attributes of the nodes `'club'` values to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "karate = pd.DataFrame(index=K.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "karate['club'] = [K.nodes[i]['club'] for i in karate.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>club</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr. Hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. Hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr. Hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. Hi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     club\n",
       "0  Mr. Hi\n",
       "1  Mr. Hi\n",
       "2  Mr. Hi\n",
       "3  Mr. Hi\n",
       "4  Mr. Hi"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Officer    17\n",
       "Mr. Hi     17\n",
       "Name: club, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karate.club.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q09'></a>\n",
    "\n",
    "\n",
    "### Question 09:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "After we have the club feature established, we aim to use features of the graph to translate into new features in our `DataFrame`.  To start, we can determine the degree of each vertex and incorporate these as features in our data.\n",
    "\n",
    "In the `karate` dataframe, create a new feature named `degree`  and fill it with the degree of each vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DegreeView({0: 16, 1: 9, 2: 10, 3: 6, 4: 3, 5: 4, 6: 4, 7: 4, 8: 5, 9: 2, 10: 3, 11: 1, 12: 2, 13: 5, 14: 2, 15: 2, 16: 2, 17: 2, 18: 2, 19: 3, 20: 2, 21: 2, 22: 2, 23: 5, 24: 3, 25: 3, 26: 2, 27: 4, 28: 3, 29: 4, 30: 4, 31: 6, 32: 12, 33: 17})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "karate['degree'] = [deg[1] for deg in K.degree()]\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 09",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>club</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     club  degree\n",
       "0  Mr. Hi      16\n",
       "1  Mr. Hi       9\n",
       "2  Mr. Hi      10\n",
       "3  Mr. Hi       6\n",
       "4  Mr. Hi       3"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "    <img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Complete_graph_K3.svg/320px-Complete_graph_K3.svg.png\" width = 20%/>\n",
    "    </center>\n",
    "    \n",
    "    \n",
    "From [*Wikipedia*](https://en.wikipedia.org/wiki/Clustering_coefficient):\n",
    "\n",
    "> *\"In graph theory, a clustering coefficient is a measure of the degree to which nodes in a graph tend to cluster together. Evidence suggests that in most real-world networks, and in particular social networks, nodes tend to create tightly knit groups characterized by a relatively high density of ties. This likelihood tends to be greater than the average probability of a tie randomly established between two nodes\"*\n",
    "\n",
    "$$\n",
    "{\\displaystyle C={\\frac {3\\times {\\mbox{number of triangles}}}{\\mbox{number of all triplets}}}} $$\n",
    "\n",
    "In `networkx`, we can execute this computation using the function `nx.clustering`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.15, 1: 0.3333333333333333, 2: 0.24444444444444444, 3: 0.6666666666666666, 4: 0.6666666666666666, 5: 0.5, 6: 0.5, 7: 1.0, 8: 0.5, 9: 0, 10: 0.6666666666666666, 11: 0, 12: 1.0, 13: 0.6, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 1.0, 19: 0.3333333333333333, 20: 1.0, 21: 1.0, 22: 1.0, 23: 0.4, 24: 0.3333333333333333, 25: 0.3333333333333333, 26: 1.0, 27: 0.16666666666666666, 28: 0.3333333333333333, 29: 0.6666666666666666, 30: 0.5, 31: 0.2, 32: 0.19696969696969696, 33: 0.11029411764705882}\n"
     ]
    }
   ],
   "source": [
    "print(nx.clustering(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Notice that the output of `nx.clustering` is a Python `dict` with the node labels as keys and the corresponding clustering coefficients as values. Observe also that this function iterates over the entire graph and computes all the clustering coefficients at once (this will be relevant later when you have to work with a relatively large graph).\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q10'></a>\n",
    "\n",
    "\n",
    "### Question 10:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Add a feature 'cluster_coef' to the `karate` dataframe below using the `nx.clustering()` method.\n",
    "\n",
    "**HINT: Use the method values to fill your dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "karate['cluster_coef'] = nx.clustering(K).values()\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The scenario for our karate club involves a rift in the members, and subsequent splitting into two clubs. Here we want to predict who will join the club.  Another feature that could be helpful involves the distance each node is from the two protagonists.  These individuals correspond to the two nodes of the highest degree in the graph.\n",
    "\n",
    "Below we identify the two nodes with the highest degree and save their indices in order to `sensei_1` and `sensei_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sensei_1, sensei_2 = karate.degree.nlargest(2).index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Hi'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.nodes[sensei_2]['club']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q11'></a>\n",
    "\n",
    "\n",
    "### Question 11:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "We will identify the highest degree node (i.e., `sensei_1`)  as `Officer` and the second highest (i.e., `sensei_2`) as `Mr. Hi`.  Subsequently, we want to measure the distance from each as our final features.  To do so, we can use **Djikstra's** shortest path algorithm with the `.djikstra_path_length()` method for each of the influencers.\n",
    "\n",
    "Add a feature `dist_officer` that includes the length of the shortest path from each vertex to the highest degree vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "karate['dist_officer'] = [nx.dijkstra_path_length(K, sensei_1, dist) for dist in K.nodes()]\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 11",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>club</th>\n",
       "      <th>degree</th>\n",
       "      <th>cluster_coef</th>\n",
       "      <th>dist_officer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>16</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>9</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>10</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     club  degree  cluster_coef  dist_officer\n",
       "0  Mr. Hi      16      0.150000             2\n",
       "1  Mr. Hi       9      0.333333             2\n",
       "2  Mr. Hi      10      0.244444             2\n",
       "3  Mr. Hi       6      0.666667             2\n",
       "4  Mr. Hi       3      0.666667             3"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Here we repeat the computation from the preceding question by computing the shortest path distance (as measured by number of edges) from each member to `Mr. Hi` (i.e., `sensei_2`, the node of second highest degree).\n",
    "\n",
    "Next, we add a feature `dist_mrh` to the datarame `karate` that is the length of the shortest path from each vertex to the second highest degree vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "karate['dist_mrh'] = [nx.dijkstra_path_length(K, sensei_2, member) for member in K.nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Here's what our dataframe looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>club</th>\n",
       "      <th>degree</th>\n",
       "      <th>cluster_coef</th>\n",
       "      <th>dist_officer</th>\n",
       "      <th>dist_mrh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>16</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>9</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>10</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. Hi</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     club  degree  cluster_coef  dist_officer  dist_mrh\n",
       "0  Mr. Hi      16      0.150000             2         0\n",
       "1  Mr. Hi       9      0.333333             2         1\n",
       "2  Mr. Hi      10      0.244444             2         1\n",
       "3  Mr. Hi       6      0.666667             2         1\n",
       "4  Mr. Hi       3      0.666667             3         1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "After building our features, we can see if we are in fact able to predict the `club` feature using a `LogisticRegression` classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = karate.drop('club', axis = 1)\n",
    "y = karate.club\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71428571, 0.71428571, 1.        , 1.        ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X_train, y_train, cv = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q12'></a>\n",
    "\n",
    "\n",
    "### Question 12:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Use cross-validation as above but add arguments to group data based on `y` and to use 5 folds. Save your results to `ans12` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "ans12= cross_val_score(clf, X_train, y_train, cv = 5, groups = y_train) # array of cross_val roc_auc scores\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 12",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#questions)\n",
    "\n",
    "In our Karate Club example, we were dealing with a very small community sample.  Now, we move to investigate an email network containing information about salary and department.  Here, we have a larger dataset as well as one additional attribute for each node that we can incorporate into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "G = nx.read_gpickle('data/email_prediction.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 1005\n",
      "Number of edges: 16706\n",
      "Average degree:  33.2458\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Department': 1, 'ManagementSalary': 0.0}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q13'></a>\n",
    "\n",
    "\n",
    "### Question 13:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your goal is to predict salaries for the nodes missing data.  We proceed by splitting the data into two sets; `labeled` and `not_labeled`.  \n",
    "\n",
    "Using the DataFrame loaded below, create two subsets of the frame determined by if  the management_salary is NaN or not. Assign the new dataframes to `labeled` and `not_labeled`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Question 19",
     "locked": false,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "salaries = pd.read_csv('data/salary_clean.csv', index_col = 0)\n",
    "### YOUR SOLUTION HERE\n",
    "labeled = salaries.loc[salaries['management_salary'].notnull()] # DataFrame of observations with management_salary data\n",
    "not_labeled = salaries.loc[salaries['management_salary'].isnull()] # DataFrame of observations missing labels in management_salary\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 13",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, you can build a basic `LogisticRegression` model on our data. Note that you build a split on the labeled data only, and reserve the unlabeled data for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(labeled.drop('management_salary', axis = 1),\n",
    "                                                    labeled['management_salary'], random_state = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a basic LogisticRegression classifier on the `X_train` and `y_train` sets. After using the function `predict()` we compute the `roc_auc` score from this estimator and assign it to the variable `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your roc_auc_score is : 0.639773\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train, y_train)\n",
    "preds = lgr.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print(\"Your roc_auc_score is : {:8.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifiers\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "<i>A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8590500090878918"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base estimator\n",
    "lgr = LogisticRegression()\n",
    "# bagging classifier\n",
    "bag = BaggingClassifier(lgr, random_state = 24)\n",
    "# fit \n",
    "scores = cross_val_score(bag, X_train, y_train, scoring = 'roc_auc')\n",
    "# score\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q14'></a>\n",
    "\n",
    "\n",
    "### Question 14:\n",
    "\n",
    "*5 points*\n",
    "    \n",
    "Fit the data with a bagged KNearestNeighbor estimator and save your cross-validated roc_auc score average to `bag_knn`. below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your roc_auc_score is : 0.859050\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "_knn = BaggingClassifier(knn, random_state = '5')\n",
    "score = cross_val_score(bag, X_train, y_train, scoring = 'roc_auc')\n",
    "bag_knn = score.mean()\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "print('Your roc_auc_score is : {:8.6f}'.format(bag_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 14",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Classifiers\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "<i>An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.</i>\n",
    "\n",
    "By default, the classifier is a `DecisionTreeClassifier`.  Below is an example from the [user guide](https://scikit-learn.org/stable/modules/ensemble.html#adaboost).  We will use the `AdaBoostClassifier` and the `GradientBoostingClassifier` here.  These boostings adjust themselves in different manners. AdaBoost focuses on misclassified data on each iteration and the GradientBoost focuses on the gradient of a loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524226661368589"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100, random_state=24)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv = 5, scoring = 'roc_auc')\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.871275391636263"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=24)\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5, scoring = 'roc_auc')\n",
    "scores.mean() "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
