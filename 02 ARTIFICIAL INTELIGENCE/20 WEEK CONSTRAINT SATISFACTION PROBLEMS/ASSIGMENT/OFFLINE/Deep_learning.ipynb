{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Deep Learning and Constraint Satisfaction Problems\n",
    "\n",
    "\n",
    "**_Author: Dhavide Aruliah_ and _Jacob Koehler_**\n",
    "\n",
    "**_Reviewer: Jessica Cervi_**\n",
    "\n",
    "**Expected time = 2.5 hours**\n",
    "\n",
    "**Total points = 90 points**\n",
    "\n",
    "    \n",
    "## Assignment Overview\n",
    "\n",
    "\n",
    "This assignment provides an opportunity to get used to *artificial neural networks* for supervised machine learning. We'll do this first by looking at the *perceptron* algorithm as an early example of a neural network. By building a simple program implementing the perceptron, you'll get a sense of the mathematical ideas underlying the training of neural networks. From there, you'll experiment with [Keras](https://keras.io/) as an example of a framework for building neural networks and solve a simple classification problem as well as a regression problem.\n",
    "\n",
    "You will also solve a few *Constraint Satisfaction Problems* by backtracking (including the famous $N$-Queens problem).\n",
    "\n",
    "The goals of the present assignment are to:\n",
    "+ Get used to the core terminology used with neural networks: layers, units, activation functions, etc.\n",
    "+ Implement the simplest neural network algorithm (the perceptron) to build a conceptual foundation on which neural networks are built.\n",
    "+ Solve som simple problems using a neural networks framework.\n",
    "+ Develop simple recursive functions for backtracking.\n",
    "\n",
    "This assignment is designed to build your familiarity and comfort coding in Python while also helping you review key topics from each module. As you progress through the assignment, answers will get increasingly complex. It is important that you adopt a data scientist's mindset when completing this assignment. **Remember to run your code from each cell before submitting your assignment.** Running your code beforehand will notify you of errors and give you a chance to fix your errors before submitting. You should view your Vocareum submission as if you are delivering a final project to your manager or client. \n",
    "\n",
    "***Vocareum Tips***\n",
    "- Do not add arguments or options to functions unless you are specifically asked to. This will cause an error in Vocareum.\n",
    "- Do not use a library unless you are expicitly asked to in the question. \n",
    "- You can download the Grading Report after submitting the assignment. This will include feedback and hints on incorrect questions. \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Implement deep learning and contraints satisfaction problems in Python\n",
    "- Implement the Percepetron algorithm in Python\n",
    "- Adapt the Percepetron algorithm to a deep-learning version with multiple layers\n",
    "- Use the `sklearn` `MLPClassifier`\n",
    "- Use `Keras` as a framework to solve Neural Network problems\n",
    "- Prepare and evaluate neural networks for regression\n",
    "- Solve a constraint satisfaction problem in Python: the $N$-Queens problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "## Index: \n",
    "\n",
    "####  Deep Learning and Constraint Satisfaction Problems\n",
    "\n",
    "\n",
    "- [Question 1](#q01)\n",
    "- [Question 2](#q02)\n",
    "- [Question 3](#q03)\n",
    "- [Question 4](#q04)\n",
    "- [Question 5](#q05)\n",
    "- [Question 6](#q06)\n",
    "- [Question 7](#q07)\n",
    "- [Question 8](#q08)\n",
    "- [Question 9](#q09)\n",
    "- [Question 10](#q10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Deep Learning and Constraint Satisfaction Problems\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "To begin, you will replicate one of the first models of an artificial Neural Network that came from [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) in 1957. While working on research funded by the US Defense Department, Rosenblatt investigated a straightforward approach to developing a classification model. You will construct a basic implementation of the Perceptron from scratch before moving to the Keras library.\n",
    "\n",
    "We will begin by importing the necesserary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard boilerplate for Python for data science\n",
    "%matplotlib inline\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from warnings import filterwarnings\n",
    "from numpy.testing import assert_equal \n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q01'></a>\n",
    "\n",
    "\n",
    "### Question 1:\n",
    "\n",
    "*10 points*\n",
    "    \n",
    "\n",
    "Complete the function body in the code cell below to implement the hard threshold activation function `sign` with function signature `sign(t)` as given below.\n",
    "+ Your function should follow the convention\n",
    "  $$\\mathrm{sign}(t) = \\begin{cases} +1, & t\\ge0 \\\\ -1, &t<0 \\end{cases}$$\n",
    "  for any real value $t\\in\\mathbb{R}$.\n",
    "+ Make sure your function `sign` is [*vectorized*](https://docs.scipy.org/doc/numpy/glossary.html#term-vectorization) (i.e., is a [*universal function*](https://docs.scipy.org/doc/numpy/reference/ufuncs.html) in the parlance of Numpy). That is, it should accept a Numpy array as an input and return a Numpy array of identical dimensions with entries $+1$ or $-1$ as required (i.e., the $\\text{sign}$ function should be applied elementwise to the array).\n",
    "+ Notice that `np.sign` won't work here (because `np.sign(0)==0` and you want `sign(0)==+1` instead).\n",
    "+ The function `np.where` can be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    return np.where(x >=0, +1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1, -1,  1, -1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sign(np.array([-4, 3.5, 1.2, -5.6, 0, -2.1]))\n",
    "#array([-1.,  1.,  1., -1.,  1., -1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def sign(t):\n",
    "    \"\"\"Returns +1 for t>=0, -1 otherwise\n",
    "    >>> sign(np.array([-4, 3.5, 1.2, -5.6, 0, -2.1]))\n",
    "    array([-1.,  1.,  1., -1.,  1., -1.])\n",
    "    \"\"\"\n",
    "    return np.where(t >=0, +1,-1)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 01",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q02'></a>\n",
    "\n",
    "\n",
    "### Question 2:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Complete the function `f_perceptrion(X, w, b)` below. Your function should take three arguments, `X`, `W` and `b` and should compute the perceptron according to the formula\n",
    "\n",
    "$$f_{\\mathrm{perceptron}}(x) = \\mathrm{sign}(W x^T +b),$$\n",
    "\n",
    "where $x$ is a row vector (i.e., one-dimensional array) of length $d$, $W$ is a row vector of length $d$, and $b$ is a scalar.\n",
    "\n",
    "+ You will make the function more flexible by allowing for $N\\times d$ *feature matrices* $X$ as input. In that case, the function can be computed using the same formula as above. Note that now the output is a $1\\times N$ vector rather than a $1\\times 1$ scalar).\n",
    "\n",
    "\n",
    "+ Tip: If the conventions around row & column vectors are messy, consider using `np.squeeze` to reduce two-dimensional row or column vectors to one-dimensional vectors. Numpy is very permissive about computing matrix-vector products using one-dimensional arrays.\n",
    "\n",
    "+ Tip: You will need to use the function `sign` defined in Question 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_perceptron(X, W, b):\n",
    "    z = (np.squeeze(W)@X.T) + b\n",
    "    return sign(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[ 3,  5,  2, -5,  3],\n",
    "                      [ 6,  2,  1,  8, -9],\n",
    "                      [ 4, -6, -7,  6, -9]])\n",
    "W, b = np.array([4,5,-2,0.2,1]), 1.5\n",
    "f_perceptron(X, W, b)\n",
    "#array([ 1.,  1., -1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4. ,  5. , -2. ,  0.2,  1. ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.squeeze(W)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def f_perceptron(X, W, b):\n",
    "    '''Returns sign(W X^T + b)\n",
    "    >>> X = np.array([[ 3,  5,  2, -5,  3],\n",
    "                      [ 6,  2,  1,  8, -9],\n",
    "                      [ 4, -6, -7,  6, -9]])\n",
    "    >>> W, b = np.array([4,5,-2,0.2,1]), 1.5\n",
    "    >>> f_perceptron(X, W, b)\n",
    "    \n",
    "    array([ 1.,  1., -1.])\n",
    "    '''\n",
    "    z = (np.squeeze(W)@X.T) + b\n",
    "    return sign(z)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 02",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q03'></a>\n",
    "\n",
    "\n",
    "### Question 3:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "\n",
    "Your task now is to write a function `find_misclassified` that identifies points that are misclassified by the function `f_perceptron` from above.\n",
    "\n",
    "+ The function takes four arguments:\n",
    "  + $N\\times d$ *feature matrix* `X`;\n",
    "  + *weight vector* `W` of length $d$;\n",
    "  + (scalar) *bias* `b`; and\n",
    "  + target vector `y` of length $N$ with entries $+1$ or $-1$.\n",
    "  The inputs `X`, `W`, and `b` are exactly as required for evaluating `f_perceptron`.\n",
    "  \n",
    "+ The output computed by the function `find_misclassified` is a one-dimensional Numpy array or a list of integers corresponding to rows of the input `X` that are misclassified by `f_perceptron` according to\n",
    "\n",
    "  $$f_{\\mathrm{perceptron}}(X_{k,:}, W, b) \\neq y_{k},$$\n",
    "  \n",
    "  where $X_{k,:}$ refers to the $k$th row of the $N\\times d$ matrix $X$.\n",
    "+ The row indices output of the function `find_misclassified` should be sorted in increasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_misclassified(X, y, W, b):\n",
    "    pred = f_perceptron(X,W,b)\n",
    "    misc = []\n",
    "    for idx, y_hat in enumerate(pred):\n",
    "        if y_hat != y[idx]:\n",
    "            misc.append(idx)\n",
    "            \n",
    "    return np.array(misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, b = np.array([-5,  5,  4, -3, -1, -8]), 0.47686675\n",
    "X = np.array([[ -5, -11, -13, -16, -13,  -7],\n",
    "              [ -1,  -7, -16,  13, -11,  11],\n",
    "              [  8,   7,   6, -16, -17,   0],\n",
    "               [  4,   6,  17,  14, -10,  -9],\n",
    "              [  0,  -5, -11,  15,  13,   8],\n",
    "              [ -5,  -2,   3,  10,   8,   8],\n",
    "               [ 19, -18,   6, -14,  16, -13]])\n",
    "y = np.array([ 1, -1,  1, -1,  1, -1,  1])\n",
    "find_misclassified(X, y, W, b)\n",
    "#array([3, 4, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def find_misclassified(X, y, W, b):\n",
    "    '''Returns 1D array of index values for which f_perceptron misclassifies rows of X\n",
    "    >>> W, b = np.array([-5,  5,  4, -3, -1, -8]), 0.47686675\n",
    "    >>> X = np.array([[ -5, -11, -13, -16, -13,  -7],\n",
    "    >>>               [ -1,  -7, -16,  13, -11,  11],\n",
    "    >>>               [  8,   7,   6, -16, -17,   0],\n",
    "    >>>               [  4,   6,  17,  14, -10,  -9],\n",
    "    >>>               [  0,  -5, -11,  15,  13,   8],\n",
    "    >>>               [ -5,  -2,   3,  10,   8,   8],\n",
    "    >>>               [ 19, -18,   6, -14,  16, -13]])\n",
    "    >>> y = np.array([ 1, -1,  1, -1,  1, -1,  1])\n",
    "    >>> find_misclassified(X, y, W, b)\n",
    "    array([3, 4, 6])\n",
    "    '''\n",
    "    pred = f_perceptron(X,W,b)\n",
    "    misc = []\n",
    "    for idx, y_hat in enumerate(pred):\n",
    "        if y_hat != y[idx]:\n",
    "            misc.append(idx)\n",
    "            \n",
    "    return np.array(misc)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 03",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Building the actual Perceptron Iteration\n",
    "\n",
    "Given training data $X$  and $y$ for a binary classification problem, you are now ready to implement the perceptron algorithm to determine a classifier with parameters $W$ and $b$. The basic steps are:\n",
    "\n",
    "+ Fix a random seed for the iteration (optional, but useful for reproducibility)\n",
    "+ Initialize $W$ and $b$ with some random values;\n",
    "+ Repeat the following steps, until convergence (i.e., until $\\mathcal{M}$ is empty or, optionally, if the number of iterations is too large).\n",
    "\n",
    "  + Compute the set $\\mathcal{M}$ of rows of $X$ misclassified by $f_{\\mathrm{perceptron}}(\\cdot , W, b)$\n",
    "  + Draw a row index $k$ from $\\mathcal{M}$ at random\n",
    "  + Use the $k$th row of $X$ and the $k$th entry of the label vector $y$ to update $W$ and $b$:\n",
    "  \n",
    "    $$\\begin{aligned} W &\\leftarrow W + \\eta y_{k} X_{k,:} \\\\ b &\\leftarrow b + \\eta y_{k} \\end{aligned}$$\n",
    "    (above $\\eta$ is a user-specified *learning rate*).\n",
    "    \n",
    "Below, we define a function that carries out this iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def perceptron_iteration(X, y, eta=1.0, ITMAX=1000, random_state=None):\n",
    "    '''Applies the perceptron algorithm to compute weights W and bias b associated with a binary\n",
    "    classification problem defined by N by d feature matrix X and N-vector y of labels.\n",
    "    >>> W, b = np. array([-5,  5,  4, -3, -1, -8]), 0.47686675\n",
    "    >>> X = np.array([[ -5, -11, -13, -16, -13,  -7],\n",
    "    >>>               [ -1,  -7, -16,  13, -11,  11],\n",
    "    >>>               [  8,   7,   6, -16, -17,   0],\n",
    "    >>>               [  4,   6,  17,  14, -10,  -9],\n",
    "    >>>               [  0,  -5, -11,  15,  13,   8],\n",
    "    >>>               [ -5,  -2,   3,  10,   8,   8],\n",
    "    >>>               [ 19, -18,   6, -14,  16, -13]])\n",
    "    >>> y = np.array([ 1, -1,  1, -1,  1, -1,  1])\n",
    "    >>> W, b = perceptron_iteration(X, y)\n",
    "    >>> print(W)\n",
    "    >>> print(b)\n",
    "    Converged after 15 iterations\n",
    "    [ 32.96891939   4.96204112 -18.91160225 -33.74141678  32.16552454  -7.59447954]\n",
    "    [5.73103265]\n",
    "    '''\n",
    "    np.random.seed(seed=random_state) # DO NOT CHANGE THIS LINE\n",
    "    N, d = X.shape\n",
    "    W, b = np.random.randn(d), np.random.randn(1)\n",
    "    # Determine misclassified rows of X\n",
    "    M = find_misclassified(X, y, W, b)\n",
    "    for iteration in range(ITMAX):\n",
    "        if len(M):\n",
    "            k = np.random.choice(M)\n",
    "            # stochastic gradient descent updates:\n",
    "            W, b = W + eta*y[k]*X[k,:], b + eta*y[k]\n",
    "            # Determine misclassified rows of X with new W and b\n",
    "            M = find_misclassified(X, y, W, b)\n",
    "        else:\n",
    "            break\n",
    "    if iteration < ITMAX-1:\n",
    "        print('Converged after {} iterations'.format(iteration))\n",
    "        return W, b\n",
    "    else:\n",
    "        print('Failed to converge: {}'.format(iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "## Neural networks\n",
    "\n",
    "After recognizing that the perceptron has two conceptual layers (an *input* layer and an *output* layer) that connect spaces of disparate dimensions, we can extend this model to more general functions. That is, we can build a *multi-layer perceptron* with multiple input layers, each associated with their own weight matrix, bias vector, and activation function. It is possible to use more general *network architectures* with these components to represent more sophisticated functions. When multiple hidden layers are used, the network is often considered as a \\\"deep\\\" neural network, hence the term *deep learning*\n",
    "\n",
    "A key component of this is the choice of the *activation function*.\n",
    "\n",
    "+ For binary classification problems, the logistic activation function\n",
    "  $$ \\sigma(t) = \\frac{e^{t}}{1+e^t} $$\n",
    "  \n",
    "  (as seen from logisitc regression) is useful for probabilities of belonging to one of the two classes.\n",
    "+ For multiclass classification problems, the $\\text{softmax}$ function is often used to provide probabilities of belonging to any of a number of classes. It is a mapping $x \\mapsto \\mathrm{softmax}(x)$ of a vector of length $d$ to another vector of length $d$ defined by\n",
    "\n",
    "  $$[\\mathrm{softmax}(x)]_{k} :=\\frac{\\exp(x_k)}{\\sum_{i=1}^{d} \\exp(x_i)} \\qquad(k=1,2,\\dotsc,d).$$\n",
    "  \n",
    "  Notice that the non-negative entries of $\\mathrm{softmax}(x)$ add up to $1$ so it is, in effect, a discrete probability mass function.\n",
    "+ The *ReLU* (\"rectified linear unit\") function is a piecewise linear function defined by\n",
    "  $$\\mathrm{relu}(t) = \\begin{cases} t, & t\\ge0 \\\\ 0, &t<0 \\end{cases}.$$\n",
    "  The ReLU function is often used in between internal layers of a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q04'></a>\n",
    "\n",
    "\n",
    "### Question 4:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Complete the Python function `softmax` below that accepts a `NumPy` array as input and return a `NumPy` array of identical dimensions with appropriate real-valued entries required according to the formula:\n",
    "\n",
    "\n",
    "  $$[\\mathrm{softmax}(x)]_{k} = \\frac{\\exp(x_k - M)}{\\sum_{i=1}^{d} \\exp(x_i-M)}.$$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    max_ = max(x)\n",
    "    exp_ = np.exp(x - max_)\n",
    "    \n",
    "    return exp_ / sum(exp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.16537670e-01, 7.85224641e-04, 8.61103378e-01, 1.57716585e-02,\n",
       "       5.80206892e-03])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3,-2,5,1,0])\n",
    "softmax(x)\n",
    "#array([1.16537670e-01, 7.85224641e-04, 8.61103378e-01, 1.57716585e-02, 5.80206892e-03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def softmax(x):\n",
    "    '''Returns smoothed version of max. function\n",
    "    >>> x = np.array([3,-2,5,1,0])\n",
    "    >>> softmax(x)\n",
    "    array([1.16537670e-01, 7.85224641e-04, 8.61103378e-01, 1.57716585e-02, 5.80206892e-03])\n",
    "    '''\n",
    "    max_ = max(x)\n",
    "    exp_ = np.exp(x - max_)\n",
    "    \n",
    "    return exp_ / sum(exp_)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 04",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q05'></a>\n",
    "\n",
    "\n",
    "### Question 5:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "You task here is to complete the *rectified linear unit* or \"ReLU\" function `relu()`\n",
    "\n",
    "The function `relu` should should accept a `NumPy` array as perform a transformation according to the convention\n",
    "\n",
    "  $$\\mathrm{relu}(t) = \\begin{cases} t, & t\\ge0 \\\\ 0, &t<0 \\end{cases}$$\n",
    "  \n",
    "  for any real value $t\\in\\mathbb{R}$.\n",
    "  \n",
    "**HINT:** The function `np.where` can be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(t):\n",
    "    return np.where(t >= 0, t, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10496716, 0.39051915, 2.29579808, 0.        , 0.42348426,\n",
       "       0.59049302])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([ 0.10496716,  0.39051915,  2.29579808, -0.22517898,  0.42348426,  0.59049302])\n",
    "relu(x)\n",
    "#array([0.10496716, 0.39051915, 2.29579808, 0.        , 0.42348426,       0.59049302])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def relu(t):\n",
    "    '''Returns t for t>=0, zero otherwise\n",
    "    >>> x = np.array([ 0.10496716,  0.39051915,  2.29579808, -0.22517898,  0.42348426,  0.59049302])\n",
    "    \n",
    "    array([0.10496716, 0.39051915, 2.29579808, 0.        , 0.42348426,       0.59049302])\n",
    "    '''\n",
    "    return np.where(t >= 0, t, 0)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 05",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "## MNIST Digit Classification with Neural Networks\n",
    "\n",
    "In the next few exercises, you'll get a chance to solve the [MNIST handwritten digits classification problem](http://yann.lecun.com/exdb/mnist/) first using  Scikit-Learn, and then using [Keras](http://keras.io) (a framework for neural networks explicitly). \n",
    "\n",
    "The Keras package provides [utilities to work with numerous datasets](https://keras.io/datasets/), but we'll avoid those here (to limit needless network traffic). Instead, we've created a compressed Numpy file in the local `assets` folder that contains the relevant data. The next few lines extract the arrays `X_train`, `y_train`, `X_test`, and `y_test` from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "d_file = np.load('data/mnist.npz')\n",
    "X_train_orig = d_file['X_train']\n",
    "X_test_orig = d_file['X_test']\n",
    "y_train_orig = d_file['y_train'].reshape(-1,1)\n",
    "y_test_orig = d_file['y_test'].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) 47040000\n",
      "(10000, 28, 28) 7840000\n",
      "(60000, 1) 60000\n",
      "(10000, 1) 10000\n"
     ]
    }
   ],
   "source": [
    "for arr in [X_train_orig, X_test_orig, y_train_orig, y_test_orig]:\n",
    "    print(arr.shape, arr.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract a single image\n",
    "idx = 45621\n",
    "digit_image = X_train_orig[idx]\n",
    "plt.imshow(digit_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Associated label: {}'.format(y_train_orig[idx]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q06'></a>\n",
    "\n",
    "\n",
    "### Question 6:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Preprocess the features in the arrays `X_train_orig` & `X_test_orig` following the steps below:\n",
    "\n",
    "+ Reshape the three-dimensional arrays into two-dimensional arrays by using the function `np.reshape`.\n",
    "+ Rescale the integer values to be real values between 0 and 1 by dividing the arrays by 255.0 (the grayscale images have integer values between 0 & 255 by default).\n",
    "+ Bind the rescaled and reshaped training and testing arrays to `X_train` & `X_test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 784)\n",
      "X_test:  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE:\n",
    "train_n, r, c = X_train_orig.shape\n",
    "test_n, r, c = X_test_orig.shape\n",
    "X_train = X_train_orig.reshape(train_n, r*c)/255\n",
    "X_test  = X_test_orig.reshape(test_n, r*c)/255\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "### For verification:\n",
    "print('X_train: {}'.format(X_train.shape))\n",
    "print('X_test:  {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 06",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For our next step, we need to preprocess the targets `y_train_orig` & `y_test_orig` by converting them to two-dimensional arrays with one-hot encoded rows (each corresponding to a particular categorical label).\n",
    "+ We can use [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to do the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(y_train_orig.reshape(-1,1))\n",
    "y_train = encoder.transform(y_train_orig.reshape(-1,1))\n",
    "y_test = encoder.transform(y_test_orig.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### ScikitLearn `MLPClassifier`\n",
    "\n",
    "Now that the data is loaded and ready to be processed, we can feed it into a simple feed-forward neural network.\n",
    "ScikitLearn has a straightforward implementation of a *Multi-layered Perceptron* (see the documemtation for the [`MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) class).  Here, we have control over aforementioned parameters like the activation function and the learning rate.\n",
    "\n",
    "\n",
    "Below we instantiate an `MLPClassifier`using the following parameters:\n",
    "\n",
    "+ Set the argument `activation='logistic'` to choose a logistic activation function.\n",
    "+ Use `hidden_layer_sizes=(512,)` to create a single hidden layer with 512 units.\n",
    "+ Set the argument  `max_iter` parameter to `5` to limit the number of iterations (or *epochs*) of gradient descent.\n",
    "+ Use `learning_rate='constant'` to keep the learning rate fixed and set `learning_rate_init` to `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(activation='logistic', hidden_layer_sizes=(512,), max_iter=5, learning_rate='constant', learning_rate_init=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Having instantiated the `MLPClassifier`, you can fit it to the training data and assess the accuracy in the usual way with Scikit-Learn estimator `fit` and `predict` (or `score`) methods. Notice, however, that this is rather slow (about 25 seconds on a laptop with 16GB RAM).\n",
    "\n",
    "**Note**: If you experience timeouts, you may want to leave the next cell commented out (and the one that follows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(512,), learning_rate='constant',\n",
       "       learning_rate_init=0.1, max_iter=5, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# LEAVE THIS CELL COMMENTED OUT THIS CELL IF YOU EXPERINCE TIMEOUTS!\n",
    "mlp.fit(X_train, y_train)   # Fitting to the training data is not so fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.825\n",
      "Testing  accuracy: 0.830\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LEAVE THIS CELL COMMENTED OUT THIS CELL IF YOU EXPERINCE TIMEOUTS!\n",
    "y_pred = mlp.predict(X_test)  # Prediction is much faster\n",
    "# Prediction is must faster\n",
    "accuracy_train = mlp.score(X_train, y_train)\n",
    "accuracy_test = mlp.score(X_test, y_test)\n",
    "print('Training accuracy: {:5.3f}'.format(accuracy_train))\n",
    "print('Testing  accuracy: {:5.3f}'.format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Introduction to Keras\n",
    "\n",
    "<center>\n",
    "    <a href=\"http://keras.io\"><img src = \"assets/keras.png\" width = \"50%\" height = \"50%\" /></a>\n",
    "</center>\n",
    "\n",
    "Now that you have some familiarity the perceptron as a simple neural network and with the multi-layer perceptron in Scikit-Learn, you can use [Keras](http://keras.io) as a more practical framework to solve problems using neural networks. Keras is a library that provides a simple API for neural network algorithms on top of lower-level libraries like [Tensorflow](https://www.tensorflow.org/) or [Theano](https://github.com/Theano/Theano). Other high-level frameworks for neural networks (\"deep learning\") include [Chainer](https://chainer.org/) and [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We are now ready to instantiate a neural network model to solve the digits classification problem. This is referred to as specifying the *architecture* of the neural network.\n",
    "\n",
    "+ To initialize the model, we instantiate an object of the class `models.Sequential` using the default options and we bind the object to the identifier \"network\" and add two layers:\n",
    "\n",
    "    - 1) The first layer is a hidden layer using `network.add` with `layers.Dense` with the first argument to `layers.Dense` equal to `512` (for 512 units). Next, we use the keyword argument `activation='relu'` to specify the ReLU activation function for this layer.\n",
    "    - 2) Finally, we add the final output layer using `network.add` & `layers.Dense`. This layer will have 10 units and `activation='softmax'` to specify the final output of the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-0aa3d2196db4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Using Keras for Linear Regression: Boston Housing data\n",
    "\n",
    "Using Keras in a regression setting is very similar to doing so for classification examples.  We will work through a basic implementation using the Boston Housing dataset from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "d_file = np.load('data/boston.npz')\n",
    "train_data = d_file['train_data']\n",
    "test_data = d_file['test_data']\n",
    "train_targets = d_file['train_targets']\n",
    "test_targets = d_file['test_targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "To begin, we need to standardize our data.   Recall that the standard score of a sample $x$ is calculated as:\n",
    "\n",
    "$$z = \\frac{(x - \\mu)}{  s}$$\n",
    "\n",
    "\n",
    "where $\\mu$ is the mean of the training samples or zero if and $s$ is the standard deviation of the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing the Housing Features\n",
    "\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q07'></a>\n",
    "\n",
    "\n",
    "### Question 7:\n",
    "\n",
    "*5 points*\n",
    "\n",
    "Your task here is to *standardize* the features of the housing data using the transformation above.\n",
    "+ Given the two-dimensional array of features in `train_data`, replace each column by subtracting its mean and dividing by its standard deviation.\n",
    "+ Assign the results to `scaled_train` and `scaled_test`.\n",
    "+ Be sure to use the means & standard deviations from the *training* data to standardize the *testing* data. That is, the standardizing transformation can only rely on information known *a propri* from training; it cannot know statistical properties of future testing instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE:\n",
    "mean_ = np.mean(train_data, axis=0)\n",
    "std_ = np.std(train_data, axis=0)\n",
    "scaled_train = (train_data - mean_)/std_\n",
    "scaled_test = (test_data - mean_)/std_\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 07",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Preparing the Neural Network for Regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Below, we have definied a function  `build_regression()`  that takes no arguments and returns a compiled Keras model using the following criteria:\n",
    "\n",
    "\n",
    "- Uses a `Sequential` model.\n",
    "- Contains two `Dense` layers with 32 units each and `relu` activation function.\n",
    "- Contains a single `Dense` output layer with 1  unit.\n",
    "- Compiles the network using the `compile()` function with the following parameters:\n",
    " - `optimizer = 'rmsprop'`\n",
    " - `loss = 'mse'`\n",
    " - `metrics = ['mae']`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_regression():\n",
    "    '''\n",
    "    Builds a Keras Sequential model with\n",
    "    two Dense layers containing 32 units and \n",
    "    a single linear output layer.\n",
    "    '''\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(32, activation = 'relu', input_shape = (train_data.shape[1], )))\n",
    "    model.add(layers.Dense(32, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Evaluating the Neural Network for Regression\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q08'></a>\n",
    "\n",
    "\n",
    "### Question 8:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "\n",
    "Just as we did with the classification example, we can assess the accuracy of the model using the testing data.\n",
    "\n",
    "+ Prepare a model using the function `build_regression` from the previous question.\n",
    "+ Use the arrays `scaled_train` and `train_targets` to train the model using the `fit` method. Provide the keyword arguments `epochs=10`, and `batch_size=128` to tailor the number of training epochs and the number of random observations drawn in each batch within an epoch.\n",
    "+ Finally, use the `evaluate` method with the testing data `scaled_test` & `test_targets` as input. The output will be a sequence of two values: the loss and the accuracy.\n",
    "+ Assign these two values to `test_loss` and `test_acc` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "model = build_regression()\n",
    "model.fit(scaled_train, train_targets, epochs=10, batch_size=128)\n",
    "test_loss, test_acc = model.evaluate(scaled_test, test_targets)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "print('test_loss: {:9.4g}'.format(test_loss))\n",
    "print('test_acc:  {:9.4g}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 08",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Constraint Satisfaction Problems and Backtracking\n",
    "\n",
    "\n",
    "In the final portion of this assignment, you'll explore a few [Constraint Satisfaction Problems](https://en.wikipedia.org/wiki/Constraint_satisfaction_problem). From Wikipedia:\n",
    "> Constraint satisfaction problems (CSPs) are mathematical questions defined as a set of objects whose state must satisfy a number of constraints or limitations. CSPs represent the entities in a problem as a homogeneous collection of finite constraints over variables, which is solved by constraint satisfaction methods. CSPs are the subject of intense research in both artificial intelligence and operations research, since the regularity in their formulation provides a common basis to analyze and solve problems of many seemingly unrelated families. CSPs often exhibit high complexity, requiring a combination of heuristics and combinatorial search methods to be solved in a reasonable time. The Boolean satisfiability problem (SAT), the satisfiability modulo theories (SMT) and answer set programming (ASP) can be roughly thought of as certain forms of a constraint satisfaction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will start with the [eight queens puzzle](https://en.wikipedia.org/wiki/Eight_queens_puzzle) or, more precisely, its generalization the $N$ queens puzzle.\n",
    "Given an $N \\times N$ chessboard, determine all the ways in which $N$ queens can be placed on the board so that no queen is threatened by another queen. Remember that queens can move an arbitrary number of spaces along horizontal rows, vertical columns, or diagonally connected squares on the chessboard. So, another way of stating the $N$ queens problem is to place $N$ queens on an $N\\times N$ chessboard so that no two queens occupy the same row or column and so that no two queens lie along any diagonal line (i.e. at 45 degrees) of the board.\n",
    "\n",
    "The $1$-Queens problem has a single, trivial solution. You can enumerate the cases for the $2$-Queens and $3$-Queens solutions to convince yourself that no solutions exist for these cases. For the $4$-Queens problem, here is one solution: \n",
    "<center>\n",
    "    <img src = './assets/sol_4x4_b.png'>\n",
    "</center>\n",
    "\n",
    "To identify positions on the chess board, let the top left square be indexed by `(0,0)` with rows increasing downward and columns increasing to the right. For convenience sake, the positions of the $N$ queens on the board can be represented as a single list: the $k$th entry of the list represents the column location of the queen in row $k$.  For example, to represent the board above, use\n",
    "\n",
    "```python\n",
    ">>> board = [2, 0, 3, 1]\n",
    "```\n",
    "\n",
    "because the queens are positioned at coordinates `(0,2)`, `(1,0)`, `(2,3)`, & `(3,1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "#The board above and our representation\n",
    "board = [2, 0, 3, 1]\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "To get started, you need to construct a function `is_nqueens_soln` to assess whether a given function is a valid solution of the $N$-Queens problem. Recall that an invalid board has two queens in the same row, the same column, or two queens on any diagonal line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Verifying a Valid *N*-Queens Chessboard\n",
    "\n",
    "[Back to top](#Index:) \n",
    "<a id='q09'></a>\n",
    "\n",
    "\n",
    "### Question 9:\n",
    "\n",
    "*20 points*\n",
    "\n",
    "\n",
    "\n",
    "The task here is to complete the function `is_nqueens_soln` that accepts a list of length $N$ as input (the representation of a board as described above).\n",
    "+ Given the list `board` of length $N$ with entries between $0$ and $N-1$, the board is assumed to have a queen at position `(k, board[k])` for `k` $=0,1,\\dotsc,N-1$.\n",
    "+ The function returns `False` if any horizontal line, vertical line, or diagonal line on the $N\\times N$ chessboard contains more than one queen.\n",
    "+ If the entries of `board` are all between $0$ and $N-1$ and the preceding condition fails, the function should return `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "def is_nqueens_soln(board):\n",
    "    '''Returns True or False according to whether board is a valid solution of the\n",
    "    N-Queens problem (assuming board is a list of N column coordinates only).\n",
    "    INPUT:\n",
    "       board: a list of length N with column positions of queens in each row.\n",
    "              (note: board should be a permutation of integers 0 through N-1).\n",
    "    OUTPUT:\n",
    "       True or False according to whether board is a valid solution of the N-Queens\n",
    "       problem.\n",
    "    EXAMPLE:\n",
    "    >>> B1 = [1, 3, 0, 2]\n",
    "    >>> is_nqueens_soln(B1)\n",
    "    True\n",
    "    >>> B2 = [2, 0, 3, 3]\n",
    "    >>> is_nqueens_soln(B2)\n",
    "    False\n",
    "    '''\n",
    "    ##12.06.2020: Code took from: Week #20 - Office Hour with CL Carleton Smith\n",
    "    ##Reference in recorded session: 01:00:58\n",
    "    def is_attack(row, col, queens):\n",
    "        return (col in queens) or (any(abs(row - r)==abs(col - c) for r,c in enumerate(queens)))\n",
    "    \n",
    "    pos = len(board)\n",
    "    if pos < 2:\n",
    "        return True\n",
    "    \n",
    "    for row in range(pos - 1, 0, -1):\n",
    "        col = board[row]\n",
    "        if is_attack(row, col, board[:row]):\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 09",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "You now have the pieces ready to solve the $N$-Queens problem, i.e., to construct the set of all solutions of the $N$-Queens for $N\\ge 4$.\n",
    "\n",
    "You can approach the problem using a general algorithm for solving CSPs called [*backtracking*](https://en.wikipedia.org/wiki/Backtracking). From Wikipedia:\n",
    "\n",
    "> Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons a candidate (\"backtracks\") as soon as it determines that the candidate cannot possibly be completed to a valid solution.\n",
    "\n",
    "\n",
    "The strategy is to enumerate incrementally a set of partial candidates that, in principle, could be completed in various ways to yield all the possible solutions to the given problem. The partial candidates can be conceptually represented as the nodes of a tree structure, the potential search tree. The backtracking algorithm traverses this search tree recursively in a depth-first order. At each node, the algorithm checks for a valid solution. If the current node cannot be completed to a valid solution, the whole subtree rooted at the current node is pruned. Otherwise, the algorithm recursively enumerates all subtrees of from the current node.\n",
    "\n",
    "In the current context, you start with an empty board and the number of queens $N$ to be placed on an $N\\times N$ board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Determining a Flight Itinerary \n",
    "\n",
    "An example of a more useful problem one can solve using backtracking is the *flight itinerary problem*. In this problem, you are given a set of tuples of the form *(origin, destination)* where each ordinate in the tuple is an airport code. A starting airport is specifed, but the set of tuples is otherwise not provided in any particular sequence. The goal is to use the list of tuples (flights) provided to recover the sequence of airports visited in sequence making sure to exhaust every tuple in the set provided. Of course, this can be considered a graph teraversal problem where the nodes/vertices are the airports and the edges are the connecting flights.\n",
    "\n",
    "For example, given the following set of flights\n",
    "```\n",
    "    ORD  EWR\n",
    "    YVR  SFO\n",
    "    SFO  ORD\n",
    "    YUL  YVR\n",
    "\n",
    "```\n",
    "\n",
    "and the starting airport `YUL`, you should recover the sequence `YUL  YVR  SFO  ORD  EWR`. In Python, you'll represent the flights as tuples of strings and the final sequence as a single list of strings:\n",
    "```python\n",
    ">>> flights = [('ORD','EWR'), ('YVR','SFO'), ('SFO','ORD'), ('YUL','YVR')]\n",
    ">>> # Eventual itinerary to arrive at...\n",
    ">>> itinerary = ['YUL', 'YVR', 'SFO', 'ORD', 'EWR']\n",
    "```\n",
    "Much like the approach to the $N$-Queens puzzle, you will start with a given itinerary, move through the list and test for valid connections.  If it is, we will continue down the list until we reach a terminal state.  If we reach an invalid move,  we will `.pop()` the move and continue on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q10'></a>\n",
    "\n",
    "\n",
    "### Question 10:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Your task here is to construct a function `get_itinerary` that returns a flight itinerary as described above from a sequence of connecting flights.\n",
    "+ The function accepts two inputs: `flights`, a list of tuples of strings of the form `(origin, destination)` (airport codes) and `itinerary`, a list of strings (airport codes) as input.\n",
    "+ The result returned should be a list of strings like `itinerary` describing a path that traverses all the nodes in `flights`.\n",
    "+ If no such path exists, it should return the Python value `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def get_itinerary(flights, itinerary):\n",
    "    '''Returns a list of airports (vertices) comprising a traversal of the edges\n",
    "    listed in the input flights.\n",
    "    INPUT:\n",
    "      flight: list of tuples of the form (origin, destination) (i.e., airports)\n",
    "      itinerary: list of destinations (airports)\n",
    "    OUTPUT:\n",
    "      list of airports traversing all edges in flights or None\n",
    "    EXAMPLE:\n",
    "    >>> flights = [('ORD','EWR'), ('YVR','SFO'), ('SFO','ORD'), ('YUL','YVR')]\n",
    "    >>> print(get_itinerary(flights, ['YUL']))\n",
    "    ['YUL', 'YVR', 'SFO', 'ORD', 'EWR']\n",
    "    >>> print(get_itinerary(flights, ['SFO']))\n",
    "    None\n",
    "    '''\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "    # If flights is empty, return itinerary (you're done)\n",
    "    if not flights:\n",
    "        return itinerary\n",
    "    \n",
    "    # Extract the previous stop from the itinerary\n",
    "    previous_stop = itinerary[-1]\n",
    "    # Loop over the list of flights:\n",
    "    for idx, (origin, destination) in enumerate(flights):\n",
    "        # Copy flights excluding current one to mark it as used\n",
    "        copy_flights = flights[:idx] + flights[idx+1:]\n",
    "        # Append the destination (second ordinate) from tuple to itinerary\n",
    "        itinerary.append(destination)\n",
    "        # When the origin (1st ordinate) matches the previous stop, return\n",
    "        #    the result of a recursive call to get_itinerary using the current\n",
    "        #    itinerary and the copy of flights that excludes the current one.)\n",
    "        if origin == previous_stop:\n",
    "            return get_itinerary(copy_flights, itinerary)\n",
    "        # Pop the last entry from the itinerary\n",
    "        itinerary.pop()\n",
    "    # Return None if the loop terminates without returning\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
